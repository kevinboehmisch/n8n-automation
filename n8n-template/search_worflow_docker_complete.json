{
  "name": "My workflow",
  "nodes": [
    {
      "parameters": {
        "options": {}
      },
      "id": "e4095546-becd-4a66-a9ef-1912029ed5da",
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        1960,
        280
      ]
    },
    {
      "parameters": {
        "jsonMode": "expressionData",
        "jsonData": "={{ $('HTTP Request8').item.json.results[0].markdown }}",
        "options": {
          "metadata": {
            "metadataValues": [
              {
                "name": "url",
                "value": "={{ $('HTTP Request8').item.json.results[0].url }}"
              },
              {
                "name": "query",
                "value": "={{ $('extract url and query').item.json.query }}"
              },
              {}
            ]
          }
        }
      },
      "id": "763ac010-a986-43c8-8336-617d291fc514",
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "typeVersion": 1,
      "position": [
        3400,
        -200
      ]
    },
    {
      "parameters": {
        "chunkSize": 700,
        "chunkOverlap": 100
      },
      "id": "d54d8dff-7054-405f-af51-28bf74a47512",
      "name": "Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter",
      "typeVersion": 1,
      "position": [
        3540,
        -60
      ]
    },
    {
      "parameters": {
        "mode": "insert",
        "tableName": {
          "__rl": true,
          "value": "documents",
          "mode": "list",
          "cachedResultName": "documents"
        },
        "options": {
          "queryName": "match_documents"
        }
      },
      "id": "3636ae01-565e-49a4-a6d7-20124983bf78",
      "name": "Supabase Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "typeVersion": 1,
      "position": [
        3300,
        -460
      ],
      "alwaysOutputData": true,
      "retryOnFail": false,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "content": "## 1.1 Set the search parameters\n\nIn the first step the search parameters can be set.\nThe LLM will automatically generate different search request variations\n\n- use **Set search request** to set the search parameter\n- you can prompt the amount of variations in the **Get different search variations** node change the amount:  `1–2 Strings` to your liking\n\n",
        "height": 234,
        "width": 590,
        "color": 7
      },
      "id": "5c77b46a-607b-4e90-8d75-6a3330e0a31b",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        0
      ]
    },
    {
      "parameters": {
        "fieldToSplitOut": "results[0].markdown",
        "options": {
          "destinationFieldName": "documents"
        }
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        2420,
        300
      ],
      "id": "b56bd9fc-5dc2-49c1-815c-c9ec0c8acbea",
      "name": "Split Out1"
    },
    {
      "parameters": {
        "model": "mxbai-embed-large:latest"
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "typeVersion": 1,
      "position": [
        3220,
        -200
      ],
      "id": "d990d676-5182-4efe-9254-4997bacd8d91",
      "name": "Embeddings Ollama",
      "credentials": {
        "ollamaApi": {
          "id": "zIemgVIGwcb5Fzlv",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "schemaType": "manual",
        "inputSchema": "{\n  \"type\": \"object\",\n  \"properties\": {\n    \"variations\": {\n      \"type\": \"array\",\n      \"items\": { \"type\": \"string\" }\n    }\n  },\n  \"required\": [\"variations\"]\n}"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.2,
      "position": [
        560,
        460
      ],
      "id": "3d20166b-fa27-42e7-9dfc-e6065c0493ff",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "url": "http://searxng:8080/search",
        "sendQuery": true,
        "specifyQuery": "=keypair",
        "queryParameters": {
          "parameters": [
            {
              "name": "=q",
              "value": "={{ $json.variations }}"
            },
            {
              "name": "format",
              "value": "json"
            }
          ]
        },
        "options": {
          "batching": {
            "batch": {
              "batchSize": 1
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1100,
        280
      ],
      "id": "06dee2e8-0394-45a7-a156-f26f0d2d3c07",
      "name": "searxng"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        360,
        460
      ],
      "id": "5811ad14-514c-4a02-b5d5-7f9247a25cd7",
      "name": "Google Gemini Chat Model2",
      "credentials": {
        "googlePalmApi": {
          "id": "7F06cMZRr0J6TFQc",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "fieldToSplitOut": "output.variations",
        "options": {
          "destinationFieldName": "variations"
        }
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        820,
        280
      ],
      "id": "4a2afcfd-ffad-4ea0-afb8-c0d8793f0bd2",
      "name": "Split Out2"
    },
    {
      "parameters": {
        "fieldToSplitOut": "results",
        "include": "selectedOtherFields",
        "fieldsToInclude": "query",
        "options": {
          "destinationFieldName": "results"
        }
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        1280,
        280
      ],
      "id": "cb6cbcf1-9382-45a9-a94f-52d80484daed",
      "name": "Split Out3"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "14facf3f-b95f-43db-8ffa-cf3ded5ba5e0",
              "name": "topic",
              "value": "e-mobility trends",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        180,
        280
      ],
      "id": "3f0197a7-ea03-48e2-b0f7-19be6980b6e0",
      "name": "Set search request"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Du bist ein Assistant, der mir hilft, für eine Suchanfrage alle sinnvollen Varianten (Synonyme, englische/ deutsche Schreibweisen, Zusammen‑/Getrenntschreibung) auszuspielen. \nEingabe: ein Thema {{ $json.topic }}. \nAusgabe: **nur** ein gültiges JSON‑Objekt mit genau einem Feld \"variations\", das eine Liste von 1–2 Strings enthält.\nBeispiel‑Schema:\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"variations\": {\n      \"type\": \"array\",\n      \"items\": { \"type\": \"string\" }\n    }\n  },\n  \"required\": [\"variations\"]\n}\n\n**Keine** zusätzlichen Erklärungen oder Floskeln, **nur** das JSON.\n",
        "hasOutputParser": true
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.6,
      "position": [
        360,
        280
      ],
      "id": "2293550d-9862-40d4-b4a4-850c039c53ff",
      "name": "Get different search variations",
      "alwaysOutputData": true,
      "retryOnFail": true,
      "waitBetweenTries": 5000,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "content": "## 1.2 Search for URLs with Searxng\n\nIt is necessary to run the searxng docker: https://github.com/searxng/searxng-docker\nto run on windows you also need to run the docker desktop app.\n\n1. edit the settings.yml:\n```\nuse_default_settings: true\n\nserver:\n  # change this key\n  secret_key: \"test\"\n  # disable limiter, otherwise problems will occure \n  limiter: false\n  public_instance: false\n\n# load the default json endpoint (set per default, just to make sure)\nsearch:\n  formats:\n    - html\n    - json\n\n# DOI‑Resolver, for scientific papers\ndoi_resolvers:\n  oadoi.org:   'https://oadoi.org/'\n  doi.org:     'https://doi.org/'\ndefault_doi_resolver: 'oadoi.org'\n```\n2. in the searxng root terminal run docker compose up \n3. set the limit how many websites you want to search\n\n### 4. Default URL:\n- the default url is specified for starting n8n via docker: `http://searxng:8080/search` \n- if you run n8n on your pc via `npx n8n` instead use: `http://127.0.0.1:8080/search` \n\n### 5. add parameters to the searching node:\n- click on **Add parameter** in the searxng node\n- Name: `categories`\n- Value: `news ` for news, `science` for research papers\n",
        "height": 814,
        "width": 670,
        "color": 7
      },
      "id": "87eb9cbd-b894-43fb-8cbf-ad9e843082a4",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        1100,
        520
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "c9aa5a9e-fffe-4cca-bd02-7c15b114f5d5",
              "name": "results.url",
              "value": "={{ $json.results.url }}",
              "type": "string"
            },
            {
              "id": "076427e6-c2cd-43f5-ba8b-3abc39fc491b",
              "name": "query",
              "value": "={{ $json.query }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1460,
        280
      ],
      "id": "3f8e6abd-ee59-4244-8c68-98818e7c5a0b",
      "name": "extract url and query"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://crawl4ai:11235/crawl",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"urls\": [\"{{ $json.results.url }}\"],\n  \"priority\": 10\n}\n",
        "options": {}
      },
      "id": "bd917215-15d9-472a-8857-450ed54fb079",
      "name": "HTTP Request8",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        2240,
        300
      ]
    },
    {
      "parameters": {
        "content": "## 1.3 Scrape the website with Crawl4ai\n\n### 1. Only needed if not using the provided docker image\n- Download the docker files: https://docs.crawl4ai.com/core/docker-deployment/\n- follow the steps as shown in the documentation for local development and build and run locally with `docker compose up --build -d`\n\n### 3. Default URL:\n- the default url is specified for starting n8n via docker: `http://crawl4ai:11235/crawl` \n- if you run n8n on your pc via `npx n8n` instead use: `http://127.0.0.1:11235/crawl` \n\n\n",
        "height": 314,
        "width": 690,
        "color": 7
      },
      "id": "e6714f97-d5a8-48c0-b085-13d3a3d46c22",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        1920,
        520
      ]
    },
    {
      "parameters": {
        "content": "## 1.4 Store the websites in the Vector Database\n\n1. To store the data of the webiste in a vector database open a bash terminal and enter postgres:\n```\n docker exec -it postgres psql -U postgres -d postgres\n```\n2. copy and run the following code, specified for embedding with ollama:\n```\n-- 1) pgvector-Extension (falls nicht bereits aktiv)\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- 2) Neue Tabelle mit 1024-dimensionalem Embedding\nCREATE TABLE documents (\n  id        bigserial PRIMARY KEY,\n  content   text,\n  metadata  jsonb,\n  embedding vector(1024)\n);\n\n-- 3) IVFFlat-Index für schnelle Ähnlichkeitssuche\nCREATE INDEX ON documents\nUSING ivfflat (embedding vector_l2_ops)\nWITH (lists = 100);\n\n-- 4) Funktion für 1024-dim-Vektoren anlegen\nCREATE OR REPLACE FUNCTION match_documents (\n  query_embedding vector(1024),\n  match_count    int      DEFAULT NULL,\n  filter         jsonb    DEFAULT '{}'\n) RETURNS TABLE (\n  id         bigint,\n  content    text,\n  metadata   jsonb,\n  similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n    SELECT\n      docs.id        AS id,\n      docs.content   AS content,\n      docs.metadata  AS metadata,\n      1 - (docs.embedding <=> query_embedding) AS similarity\n    FROM documents AS docs\n    WHERE docs.metadata @> filter\n    ORDER BY docs.embedding <=> query_embedding\n    LIMIT match_count;\nEND;\n$$;\n```\n- leave the sql editor with `exit`\n\n\n### 2. Default URL for Ollama Credentials:\n- Pull `ollama pull mxbai-embed-large:latest` and make sure the ollama server is already running with `ollana serve`\n- the default url is specified for starting n8n via docker: `hhttp://ollama:11434` \n- if you run n8n on your pc via `npx n8n` instead use: `http://localhost:11434` ",
        "height": 1094,
        "width": 690,
        "color": 7
      },
      "id": "3b70861f-1cd9-48fc-b5b8-38a7846219c6",
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        3160,
        280
      ]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Basierend auf den vorliegenden Dokumenten und deinem Expertenwissen, beantworte diese Frage:\n\n{{ $json.frage }}",
        "options": {
          "systemPromptTemplate": "=Du bist Research-Assistant™, Dr.-Level in E-Mobilität.\n\n# Regeln:\n\n1. Nutze ausschließlich diesen Kontext:\n\n{context}\n\n2. Beantworte dann die Frage:\n\n{{ $json.frage }}\n\n3. verwende nur die relevanteste quelle und merke dir die url aus den metadaten, gib nur eine konkrete url zurück! \n\n3. Antworte nur im JSON. Befülle folgende Felder:\n\n- frage (string)\n- thema (string)\n- url (string)\n- zusammenfassung (string)\n- analyse (string)\n- ranking (string)\n\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainRetrievalQa",
      "typeVersion": 1.5,
      "position": [
        1900,
        1860
      ],
      "id": "8073fc23-d55a-476e-9fef-805fe9d0e4d2",
      "name": "Question and Answer Chain",
      "alwaysOutputData": true,
      "retryOnFail": true,
      "waitBetweenTries": 5000,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "topK": 20
      },
      "type": "@n8n/n8n-nodes-langchain.retrieverVectorStore",
      "typeVersion": 1,
      "position": [
        2020,
        2140
      ],
      "id": "f4581d48-4348-4d26-93a4-67685d115a28",
      "name": "Vector Store Retriever"
    },
    {
      "parameters": {
        "tableName": {
          "__rl": true,
          "value": "documents",
          "mode": "list",
          "cachedResultName": "documents"
        },
        "options": {
          "queryName": "match_documents"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "typeVersion": 1.1,
      "position": [
        3860,
        -180
      ],
      "id": "59c89eb0-6cf2-4a72-bcf6-a2c89f9af4da",
      "name": "Supabase Vector Store4"
    },
    {
      "parameters": {
        "model": "mxbai-embed-large:latest"
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "typeVersion": 1,
      "position": [
        1980,
        2540
      ],
      "id": "0939c925-b8d0-4649-9810-8e01faa9a900",
      "name": "Embeddings Ollama3",
      "credentials": {
        "ollamaApi": {
          "id": "zIemgVIGwcb5Fzlv",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {
          "temperature": 0.2
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        1880,
        2140
      ],
      "id": "b86b2850-2a0e-446f-ba02-c7d45c922dbb",
      "name": "Google Gemini Chat Model5",
      "credentials": {
        "googlePalmApi": {
          "id": "7F06cMZRr0J6TFQc",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "maxItems": 100
      },
      "type": "n8n-nodes-base.limit",
      "typeVersion": 1,
      "position": [
        1640,
        280
      ],
      "id": "100d0aa7-92ea-4235-b749-e703e71ce9da",
      "name": "Limit"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "00eb5335-583c-4756-aa39-6e79701b8495",
              "name": "question",
              "value": "Welche Trends, Technologien und Rahmenbedingungen sind aktuell und in den nächsten Jahrzehnten relevant für die Entwicklung der Elektromobilität in Deutschland? Berücksichtigen Sie dabei Infrastruktur (Lade-Netze, Stromversorgung), Politik und Regulierung (Förderprogramme, Gesetzgebung), Wirtschaft (Marktentwicklung, Industriepartnerschaften), Technologie (Batterie-Innovation, Fahrzeugdesign) und gesellschaftliches Nutzungsverhalten.",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1000,
        1860
      ],
      "id": "a6ae5b78-1244-439f-9f69-523524ae5866",
      "name": "Theme Question"
    },
    {
      "parameters": {
        "content": "# 1. Vectorstorage Workflow\n\n## Get and store data for topic: e-mobility\n\nThis is the workflow just to get the data and store it.\nAnalyzation will be done in the 2. workflow\n\n",
        "height": 220,
        "width": 490,
        "color": 4
      },
      "id": "218ece02-d501-4989-9c8b-cd3c206a3a0e",
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -560,
        200
      ]
    },
    {
      "parameters": {
        "content": "## 2.2 Set the analysing question and rank the best data\n\n- use **Theme Question** to edit the input question\n- Number of results by the Vector Store can be set in the **Vector Store Retreiver** Node\n",
        "height": 214,
        "width": 530,
        "color": 7
      },
      "id": "95c40b0b-f311-42ca-a3c7-3d56bd6e847b",
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        960,
        1440
      ]
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "triggerAtHour": 7
            }
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [
        -20,
        280
      ],
      "id": "ff1d1641-ac9d-46a0-adc1-841e0a8701c7",
      "name": "Schedule Trigger"
    },
    {
      "parameters": {
        "content": "# 2. Analyzing workflow \n\n## Analyze the data\n\nThis is the workflow is necessary for the data analyzation. Simple with just one node\n\n\n",
        "height": 220,
        "width": 490,
        "color": 5
      },
      "id": "d9fd9210-c153-43de-b4c8-281c87b227b1",
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -540,
        1440
      ]
    },
    {
      "parameters": {
        "content": "## Experimental: Analyze the data with an AI Agent \n\nThis is the workflow is necessary for the data analyzation, implemented with an AI Agent who calls a sub-workflow\n\n\n",
        "height": 140,
        "width": 490,
        "color": 5
      },
      "id": "930c7995-c02e-49b9-87d7-b8cb978433de",
      "name": "Sticky Note11",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -560,
        4860
      ]
    },
    {
      "parameters": {
        "content": "## 3. Experimental: Sub-workflow for the AI Agent\n\nThis is the sub-workflow which gets the RAG Data, it needs to be copied in an extra workflow file outside of this one\n",
        "height": 140,
        "width": 490,
        "color": 6
      },
      "id": "9209e67c-bc59-439f-82d9-73714ca15785",
      "name": "Sticky Note12",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        900,
        4860
      ]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Basierend auf den vorliegenden Dokumenten und deinem Expertenwissen, beantworte diese Frage:\n\n{{ $json.question }}\n\nGib mir die Antwort als JSON-Array zurück.\n",
        "hasOutputParser": true,
        "options": {
          "systemMessage": "Du bist Research-Assistant™, Dr.-Level in E-Mobilität.  \nWenn Du Kontext brauchst, rufe **search_docs(query, 10)** auf. Du erhältst ein Array von 10 Dokumenten, jedes mit mindestens diesen Feldern:\n- url     (string)\n- title   (string)\n- snippet (string – kurzer Auszug)\n\nArbeite so:\n\n1. Rufe **search_docs({{ $json.parameters.query }}, 10)** auf (schreibe im Stream-of-Thought: „I will call search_docs because…“).  \n2. Ordne jedem der 10 Treffer einen eigenen Trend zu.  \n3. Für jedes Ergebnis baue ein JSON-Objekt mit diesen Feldern:\n   - **uebertheme**: immer „E-Mobilität“  \n   - **theme**: kurzer Trend-Name (z. B. „Plug and Charge“)  \n   - **url**: exakte URL aus dem RAG-Treffer  \n   - **analysis**: 2–3 Sätze, die erklären, **warum gerade dieses Dokument** den Trend untermauert (verweise auf Title oder Snippet)  \n   - **summary**: 1–2 Sätze prägnanter Zusammenfassung des Dokuments  \n   - **ranking**: Position 1–10 entsprechend Relevanz (1 = höchst relevant)  \n   - **date**: heutiges Datum im Format YYYY-MM-DD\n\n4. Gib **nur** ein JSON-Array mit genau 10 solchen Objekten zurück – sonst nichts.\n",
          "maxIterations": 10
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 1.9,
      "position": [
        460,
        4860
      ],
      "id": "e635053c-103d-4dcc-b32b-6c547336f9bd",
      "name": "AI Agent"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        400,
        5080
      ],
      "id": "8bd2a961-8788-4222-9797-2056daf3ec30",
      "name": "Google Gemini Chat Model"
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {}
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [
        20,
        4860
      ],
      "id": "7b5573d4-b2df-465e-bb49-fe5fe467a8d3",
      "name": "Schedule Trigger2"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "00eb5335-583c-4756-aa39-6e79701b8495",
              "name": "question",
              "value": " Welche Trends sind für E-Mobilität in Deutschland in den nächsten 40 Jahren relevant?",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        240,
        4860
      ],
      "id": "56f43fe4-2f75-4200-92b5-6c422f0dda3c",
      "name": "Theme Question2"
    },
    {
      "parameters": {
        "name": "search_docs",
        "description": "search the vector data base to answer the question.",
        "workflowId": {
          "__rl": true,
          "value": "0kjMhRTU8aA2NSAb",
          "mode": "list",
          "cachedResultName": "search_docs"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "name": "search_docs",
            "description": "Search the vector database for semantically similar documents",
            "parameters": "={\n  \"query\": \"{{ $fromAI('query', '', 'string') }}\"\n}\n"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "name",
              "displayName": "name",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string",
              "removed": false
            },
            {
              "id": "description",
              "displayName": "description",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "string",
              "removed": false
            },
            {
              "id": "parameters",
              "displayName": "parameters",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "canBeUsedToMatch": true,
              "type": "object",
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        }
      },
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "typeVersion": 2.1,
      "position": [
        560,
        5080
      ],
      "id": "a3b9f8cb-e680-41c1-a42c-48d84fb600eb",
      "name": "search_docs2"
    },
    {
      "parameters": {
        "jsonSchemaExample": "[\n  {\n    \"uebertheme\": \"E-Mobilität\",\n    \"theme\": \"Plug and Charge\",\n    \"url\": \"https://goingelectric.de/plug-and-charge-pilotprojekt\",\n    \"analysis\": \"Dieses Pilotprojekt beschreibt im Detail, wie ISO 15118 die Ladeauthentifizierung automatisiert – besonders interessant sind die Feldtests in München und Berlin, die im Artikel erläutert werden.\",\n    \"summary\": \"Plug and Charge ermöglicht das automatische Starten und Abrechnen von Ladevorgängen ohne zusätzliche Hardware oder App.\",\n    \"ranking\": 1,\n    \"date\": \"2025-04-25\"\n  },\n  {\n    \"uebertheme\": \"E-Mobilität\",\n    \"theme\": \"Bidirektionales Laden\",\n    \"url\": \"https://bdew.de/v2g-studie-2024\",\n    \"analysis\": \"Die BDEW-Studie zeigt anhand von Simulationen für das Berliner Netz, wie V2G bis zu 15 % Spitzenlast abfedern kann – interessant sind die Methodik und die konkreten Zahlen zur Netzstabilität.\",\n    \"summary\": \"Bidirektionales Laden erlaubt es E-Autos, Strom ins Netz zurückzuspeisen und so Netzschwankungen auszugleichen.\",\n    \"ranking\": 2,\n    \"date\": \"2025-04-25\"\n  }\n]\n"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.2,
      "position": [
        720,
        5080
      ],
      "id": "7499e660-932e-4065-beb3-83ee71d5bcb0",
      "name": "Structured Output Parser1"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "Beantworte diese Frage mit den 10 besten Vektor-DB-Treffern und liefere eine wissenschaftlich fundierte Analyse:\n\nWelche Trends sind für E-Mobilität in Deutschland in den nächsten 40 Jahren relevant?\n",
        "options": {
          "systemPromptTemplate": "=Du bist Research-Assistant™, Dr.-Level in E-Mobilität.\nNutze ausschließlich diesen Kontext:\n\n{context}\n\nBeantworte dann die Frage:\n\n{{ $json.parameters.query }}\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainRetrievalQa",
      "typeVersion": 1.5,
      "position": [
        1780,
        4880
      ],
      "id": "d8f3d87b-4c1a-4f50-ab26-4c61e8524853",
      "name": "Question and Answer Chain1"
    },
    {
      "parameters": {
        "topK": 20
      },
      "type": "@n8n/n8n-nodes-langchain.retrieverVectorStore",
      "typeVersion": 1,
      "position": [
        1920,
        5100
      ],
      "id": "b7faf5b6-a6bc-4d7c-a39e-e2d2a80cfd6c",
      "name": "Vector Store Retriever1"
    },
    {
      "parameters": {
        "tableName": {
          "__rl": true,
          "value": "documents",
          "mode": "list",
          "cachedResultName": "documents"
        },
        "options": {
          "queryName": "match_documents"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "typeVersion": 1.1,
      "position": [
        1960,
        5300
      ],
      "id": "ba947b20-334c-4e28-8d48-6f9d6ece4a09",
      "name": "Supabase Vector Store5"
    },
    {
      "parameters": {
        "model": "mxbai-embed-large:latest"
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "typeVersion": 1,
      "position": [
        1880,
        5500
      ],
      "id": "3b17596b-1f56-4bdb-a95b-3a5cd8d54378",
      "name": "Embeddings Ollama4"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {
          "temperature": 0.2
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        1680,
        5100
      ],
      "id": "34b6b852-b07b-4ddf-90ab-218d22d5c366",
      "name": "Google Gemini Chat Model6"
    },
    {
      "parameters": {
        "inputSource": "jsonExample",
        "jsonExample": "{\n  \"name\": \"search_docs\",\n  \"description\": \"Search the vector database for semantically similar documents\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\n        \"type\": \"string\",\n        \"description\": \"The user’s question\"\n      }\n    },\n    \"required\": [\"query\"]\n  }\n}\n"
      },
      "id": "8b9f4920-6c83-4f67-8e6a-d04f6fd348c0",
      "typeVersion": 1.1,
      "name": "search_docs1",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "position": [
        1480,
        4880
      ]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Basierend auf dieser Fragestellung:\n\n{{ $json.question }}\n\ngeneriere einen prompt für die Vectordatenbank um die relevantesten Dokumente herauszufinden, fasse dich vom unfang her so kurz wie die frage selbst, nutze unterschiedliche variationen damit immer eine neue query rauskommt.\n\nKein Einleitungstext, kein schnickschnack nur eine einzige Frage!",
        "messages": {
          "messageValues": [
            {
              "message": "=Du bist ein wissenschaftler und stellst dir die frage nach den zukunftstrends der e mobilität in deutschland. dein output ist eine einzige frage vom unfang her so groß wie die übergebene frage:  {{ $json.question }} varieere und sei kreativ in der formulierung, blicke über den tellerand der ursprünglichen frage hinaus!"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.6,
      "position": [
        1260,
        1860
      ],
      "id": "86f56d86-8160-4d9d-8423-6e3a442b86a7",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        1240,
        2120
      ],
      "id": "5bcc5b39-daf8-4f53-98e4-3e201303dbf1",
      "name": "Google Gemini Chat Model1",
      "credentials": {
        "googlePalmApi": {
          "id": "7F06cMZRr0J6TFQc",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "3f8d891d-fbb8-4688-bd75-591140b4bc82",
              "name": "frage",
              "value": "={{ $json.text }}",
              "type": "string"
            },
            {
              "id": "3f676690-e721-4aeb-a7cd-d439d878ee31",
              "name": "thema",
              "value": "",
              "type": "string"
            },
            {
              "id": "df01242b-0579-4b52-983d-b56aa1cca60c",
              "name": "url",
              "value": "",
              "type": "string"
            },
            {
              "id": "97533a06-06cd-44ef-a2d4-5d6c3c599fb4",
              "name": "zusammenfassung",
              "value": "",
              "type": "string"
            },
            {
              "id": "779dcfe4-3c40-482d-b48a-2df4fc13d4df",
              "name": "analysen",
              "value": "",
              "type": "string"
            },
            {
              "id": "338f90d9-89b6-4435-b2db-9991994e6af2",
              "name": "ranking",
              "value": 0,
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1660,
        1860
      ],
      "id": "e415e142-8d88-4bca-b3f9-7855ba8fe717",
      "name": "Edit Fields"
    },
    {
      "parameters": {
        "text": "={{ $json.response }}",
        "attributes": {
          "attributes": [
            {
              "name": "frage",
              "description": "die ursprüngliche frage auf die das llm geantwortet hat",
              "required": true
            },
            {
              "name": "thema",
              "description": "das thema kurz als überschrift",
              "required": true
            },
            {
              "name": "url",
              "description": "die angegebene url",
              "required": true
            },
            {
              "name": "zusammenfassung",
              "description": "die zusammenfassung des textes",
              "required": true
            },
            {
              "name": "analysen",
              "description": "die analyse des textes",
              "required": true
            },
            {
              "name": "ranking",
              "description": "das ranking der relevanz",
              "required": true
            }
          ]
        },
        "options": {
          "systemPromptTemplate": "You are an expert extraction algorithm.\nOnly extract relevant information from the text.\nIf you do not know the value of an attribute asked to extract, leave it blank"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.informationExtractor",
      "typeVersion": 1,
      "position": [
        2320,
        1860
      ],
      "id": "a6835e6b-5025-4abc-954e-86ad1822a21e",
      "name": "Information Extractor",
      "alwaysOutputData": true,
      "retryOnFail": true,
      "waitBetweenTries": 5000,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        2380,
        2140
      ],
      "id": "a0f3b46a-cc84-4048-a1df-d728754f3e0e",
      "name": "Google Gemini Chat Model4",
      "credentials": {
        "googlePalmApi": {
          "id": "7F06cMZRr0J6TFQc",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "4ea899ff-ae1d-4dcb-8cd9-03cb799ac308",
              "name": "output.frage",
              "value": "={{ $json.output.frage }}",
              "type": "string"
            },
            {
              "id": "0fccd4e0-03e9-4874-bec9-8672ec86b722",
              "name": "output.thema",
              "value": "={{ $json.output.thema }}",
              "type": "string"
            },
            {
              "id": "08a4f0a2-1478-4e37-b4f8-1573e48787df",
              "name": "output.url",
              "value": "={{ $json.output.url }}",
              "type": "string"
            },
            {
              "id": "48d62b56-6363-4203-8fac-0965eec25dae",
              "name": "output.zusammenfassung",
              "value": "={{ $json.output.zusammenfassung }}",
              "type": "string"
            },
            {
              "id": "47f5ae98-881a-46ba-aac8-3017fe3a460b",
              "name": "output.analysen",
              "value": "={{ $json.output.analysen }}",
              "type": "string"
            },
            {
              "id": "ef909638-41a1-4776-a0c9-b432df090d0e",
              "name": "output.ranking",
              "value": "={{ $json.output.ranking }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        2880,
        1860
      ],
      "id": "56085a9c-8365-4860-a95a-7682fcb3c353",
      "name": "Edit Fields1"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        720,
        1680
      ],
      "id": "ea547c78-1497-4b58-819a-9f73d9134010",
      "name": "Loop Over Items1"
    },
    {
      "parameters": {
        "jsCode": "// Anzahl der Wiederholungen\nconst count = 7;\n\n// Erzeuge ein Array mit 'count' leeren Objekten\nconst items = [];\nfor (let i = 0; i < count; i++) {\n  items.push({ json: {} });\n}\n\n// Gib die Items zurück\nreturn items;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        20,
        1680
      ],
      "id": "44b61711-2e78-4105-ac38-cfaa8e1dc86a",
      "name": "Code"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        3120,
        1680
      ],
      "id": "210a35c4-6d13-4e1e-bced-8d1c84c39e2a",
      "name": "Merge"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Du erhältst ein JSON-Array unter dem Feld {{ JSON.stringify($json.data) }}.  \nJedes Element hat die Felder frage, thema, url, zusammenfassung, analyse, ranking.  \nWähle daraus die 2 **relevantesten** Items aus und liefere ein neues JSON-Array mit genau diesen beiden Objekten.  \nFüge jedem Objekt das Feld \"begründung\" hinzu, in dem Du in 1–2 Sätzen erklärst, warum gerade diese Quelle so gewählt wurde.\n\n\nDu erhältst ein JSON-Array unter dem Feld {{ JSON.stringify($json.data) }}. \nJedes Element hat die Felder frage, thema, url, zusammenfassung, analysen, ranking.\n\nWähle die 2 relevantesten Objekte aus, füge jedem das Feld \"begründung\" hinzu, in dem Du in 1–2 Sätzen erklärst, warum gerade diese Quelle so gewählt wurde und gib **ausschließlich** ein gültiges JSON-Array zurück.\n\n",
        "hasOutputParser": true,
        "messages": {
          "messageValues": [
            {
              "message": "=Du bist eine Expertin für Elektrotechnik und Wirtschaft am Standort Deutschland und analysierst auf Professoren-Level für die Forschung.\n"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.6,
      "position": [
        3580,
        1680
      ],
      "id": "65d38172-a875-44ab-87b8-0c772b7fe939",
      "name": "Basic LLM Chain1",
      "retryOnFail": true,
      "waitBetweenTries": 5000,
      "alwaysOutputData": true,
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-001",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        3500,
        1880
      ],
      "id": "cb281771-dbe8-4ade-a61b-de59507ff5cd",
      "name": "Google Gemini Chat Model7",
      "credentials": {
        "googlePalmApi": {
          "id": "7F06cMZRr0J6TFQc",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        3340,
        1680
      ],
      "id": "3e3f88e1-500a-4fdf-96aa-e025420b59f7",
      "name": "Aggregate"
    },
    {
      "parameters": {
        "schemaType": "manual",
        "inputSchema": "{\n  \"type\": \"array\",\n  \"minItems\": 2,\n  \"maxItems\": 2,\n  \"items\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"frage\":         { \"type\": \"string\" },\n      \"thema\":         { \"type\": \"string\" },\n      \"url\":           { \"type\": \"string\", \"format\": \"uri\" },\n      \"zusammenfassung\": { \"type\": \"string\" },\n      \"analysen\":       { \"type\": \"string\" },\n      \"ranking\":       { \"type\": \"string\" },\n      \"begründung\":    { \"type\": \"string\" }\n    },\n    \"required\": [\"frage\",\"thema\",\"url\",\"zusammenfassung\",\"analyse\",\"ranking\",\"begründung\"]\n  }\n}\n"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.2,
      "position": [
        3700,
        1880
      ],
      "id": "7e812f2c-3cc9-4057-a5b9-4fa2b4db65ee",
      "name": "Structured Output Parser2"
    },
    {
      "parameters": {
        "tableId": "quellen",
        "fieldsUi": {
          "fieldValues": [
            {
              "fieldId": "frage",
              "fieldValue": "={{ $json.output[0].frage }}"
            },
            {
              "fieldId": "thema",
              "fieldValue": "={{ $json.output[0].thema }}"
            },
            {
              "fieldId": "url",
              "fieldValue": "={{ $json.output[0].url }}"
            },
            {
              "fieldId": "zusammenfassung",
              "fieldValue": "={{ $json.output[0].zusammenfassung }}"
            },
            {
              "fieldId": "analysen",
              "fieldValue": "={{ $json.output[0].analysen }}"
            },
            {
              "fieldId": "ranking",
              "fieldValue": "={{ $json.output[0].ranking }}"
            },
            {
              "fieldId": "begründung",
              "fieldValue": "={{ $json.output[0]['begründung'] }}"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.supabase",
      "typeVersion": 1,
      "position": [
        4260,
        -200
      ],
      "id": "d6257cc2-a5cc-4e6a-ac44-7f6b31f8b7d4",
      "name": "Supabase"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        4040,
        1680
      ],
      "id": "a9272bde-9926-4516-a437-ad8876d49b61",
      "name": "Loop Over Items2"
    },
    {
      "parameters": {
        "content": "## 2.3 Let the Model analyze the best results\n\n- in this node the model analyzes the results and outputs an array only with the two best results\n- the amount of results the model gives back can be defined in the prompt\n",
        "height": 194,
        "width": 530,
        "color": 7
      },
      "id": "69b4bb44-d291-4fee-b465-8f0a63b8c246",
      "name": "Sticky Note13",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        2900,
        1440
      ]
    },
    {
      "parameters": {
        "content": "## 2.4 Store the best results in the database\n\n- it loops over the best results and stores them in the database\n- Setup the database with the following sql query in the sql editor in postgres:\n\n```\ncreate extension if not exists \"pgcrypto\";\n\n\ncreate table public.quellen (\n  id              uuid                     primary key default gen_random_uuid(),\n  frage           text                     not null,\n  thema           text                     not null,\n  url             text                     not null,\n  zusammenfassung text,       \n  analysen         text,      \n  ranking         text,\n  \"begründung\"    text,       \n  created_at      timestamp with time zone default now()\n);\n```\n- check if the table was created in the postgres editor with `\\dt public.*`\n### view output:\n- to view the output use the command `\\x auto` in the editor for a better view use `\\x off `to deactivate\n- use `SELECT * FROM public.quellen;` to see the output\n\n",
        "height": 534,
        "width": 830,
        "color": 7
      },
      "id": "42c86da8-ef94-4104-b879-b1fe1f540b74",
      "name": "Sticky Note14",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        4460,
        1660
      ]
    },
    {
      "parameters": {
        "content": "## 2.1 Setting the amount of retrieved items from the vector store\n\n- set the amount of retrieved items with `const count`\n- the amount will define how many resources the AI will analyze\n",
        "height": 214,
        "width": 530,
        "color": 7
      },
      "id": "2aaf0061-e750-46ff-8523-8a02eb995344",
      "name": "Sticky Note15",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        20,
        1440
      ]
    },
    {
      "parameters": {
        "content": "## 2.3 Let the LLM retreive Data from the Vector store\n\n- the LLM will retreive the data from supabase\n- connect supbase, gemini and ollama credentials\n",
        "height": 214,
        "width": 530,
        "color": 7
      },
      "id": "e611041a-1697-40f8-97ef-0f5d69b9d535",
      "name": "Sticky Note16",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        1880,
        1440
      ]
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.noOp",
      "name": "Replace Me",
      "typeVersion": 1,
      "position": [
        20,
        3140
      ],
      "id": "ed65bd42-28a3-462c-9025-bf53121f06e7"
    },
    {
      "parameters": {
        "content": "# 3. Continue the Workflow\n\n- to continue the workflow replace this node with the workflow 1 and 2 by copy and pasting them.\n- this way it is possible to analyze multiple different topics in one continious workflow\n- subject topics need to be changed in the `set search request` node and the `theme topic` node\n\n",
        "height": 220,
        "width": 490,
        "color": 7
      },
      "id": "24ca11e0-eb20-403b-a212-97bd73506ee4",
      "name": "Sticky Note17",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -540,
        3100
      ]
    },
    {
      "parameters": {
        "mode": "insert",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePGVector",
      "typeVersion": 1.1,
      "position": [
        2740,
        300
      ],
      "id": "0241dfa6-7e6c-445f-a773-da586e70a1e9",
      "name": "Postgres PGVector Store",
      "credentials": {
        "postgres": {
          "id": "ereWPYqdWvkladAz",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "model": "mxbai-embed-large:latest"
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "typeVersion": 1,
      "position": [
        2720,
        560
      ],
      "id": "25a850c1-c845-48af-bb5f-210be8c968e8",
      "name": "Embeddings Ollama1",
      "credentials": {
        "ollamaApi": {
          "id": "zIemgVIGwcb5Fzlv",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "typeVersion": 1,
      "position": [
        2840,
        560
      ],
      "id": "eac643fe-e199-4e55-9827-252f76f46b53",
      "name": "Default Data Loader1"
    },
    {
      "parameters": {
        "chunkSize": 700,
        "chunkOverlap": 100
      },
      "type": "@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter",
      "typeVersion": 1,
      "position": [
        2940,
        780
      ],
      "id": "fa2aece7-c228-460f-a27f-ea4138d0c5eb",
      "name": "Character Text Splitter1"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePGVector",
      "typeVersion": 1.1,
      "position": [
        2060,
        2340
      ],
      "id": "86e40500-6ea2-4b0e-94ba-62e78e3428ef",
      "name": "Postgres PGVector Store1",
      "credentials": {
        "postgres": {
          "id": "ereWPYqdWvkladAz",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "schema": {
          "__rl": true,
          "mode": "list",
          "value": "public"
        },
        "table": {
          "__rl": true,
          "value": "quellen",
          "mode": "list",
          "cachedResultName": "quellen"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "frage": "={{ $json.output[0].frage }}",
            "thema": "={{ $json.output[0].thema }}",
            "url": "={{ $json.output[0].url }}",
            "zusammenfassung": "={{ $json.output[0].zusammenfassung }}",
            "begründung": "={{ $json.output[0]['begründung'] }}",
            "ranking": "={{ $json.output[0].ranking }}",
            "analysen": "={{ $json.output[0].analysen }}"
          },
          "matchingColumns": [
            "id"
          ],
          "schema": [
            {
              "id": "id",
              "displayName": "id",
              "required": false,
              "defaultMatch": true,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": true
            },
            {
              "id": "frage",
              "displayName": "frage",
              "required": true,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "thema",
              "displayName": "thema",
              "required": true,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "url",
              "displayName": "url",
              "required": true,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "zusammenfassung",
              "displayName": "zusammenfassung",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "analysen",
              "displayName": "analysen",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "ranking",
              "displayName": "ranking",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "begründung",
              "displayName": "begründung",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "created_at",
              "displayName": "created_at",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "dateTime",
              "canBeUsedToMatch": true,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        4280,
        1880
      ],
      "id": "8b738d61-ca3c-4cd7-a842-b842f285eb8f",
      "name": "Postgres",
      "credentials": {
        "postgres": {
          "id": "ereWPYqdWvkladAz",
          "name": "Postgres account"
        }
      }
    }
  ],
  "pinData": {},
  "connections": {
    "Loop Over Items": {
      "main": [
        [
          {
            "node": "Code",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "HTTP Request8",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Default Data Loader": {
      "ai_document": [
        [
          {
            "node": "Supabase Vector Store",
            "type": "ai_document",
            "index": 0
          }
        ]
      ]
    },
    "Character Text Splitter": {
      "ai_textSplitter": [
        [
          {
            "node": "Default Data Loader",
            "type": "ai_textSplitter",
            "index": 0
          }
        ]
      ]
    },
    "Supabase Vector Store": {
      "main": [
        []
      ]
    },
    "Split Out1": {
      "main": [
        [
          {
            "node": "Postgres PGVector Store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings Ollama": {
      "ai_embedding": [
        [
          {
            "node": "Supabase Vector Store",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Get different search variations",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "searxng": {
      "main": [
        [
          {
            "node": "Split Out3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model2": {
      "ai_languageModel": [
        [
          {
            "node": "Get different search variations",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Split Out2": {
      "main": [
        [
          {
            "node": "searxng",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Out3": {
      "main": [
        [
          {
            "node": "extract url and query",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set search request": {
      "main": [
        [
          {
            "node": "Get different search variations",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get different search variations": {
      "main": [
        [
          {
            "node": "Split Out2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "extract url and query": {
      "main": [
        [
          {
            "node": "Limit",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request8": {
      "main": [
        [
          {
            "node": "Split Out1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Vector Store Retriever": {
      "ai_retriever": [
        [
          {
            "node": "Question and Answer Chain",
            "type": "ai_retriever",
            "index": 0
          }
        ]
      ]
    },
    "Supabase Vector Store4": {
      "ai_vectorStore": [
        []
      ]
    },
    "Embeddings Ollama3": {
      "ai_embedding": [
        [
          {
            "node": "Postgres PGVector Store1",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model5": {
      "ai_languageModel": [
        [
          {
            "node": "Question and Answer Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Limit": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Question and Answer Chain": {
      "main": [
        [
          {
            "node": "Information Extractor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Theme Question": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Schedule Trigger": {
      "main": [
        [
          {
            "node": "Set search request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Schedule Trigger2": {
      "main": [
        [
          {
            "node": "Theme Question2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Theme Question2": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "search_docs2": {
      "ai_tool": [
        [
          {
            "node": "AI Agent",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser1": {
      "ai_outputParser": [
        [
          {
            "node": "AI Agent",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Vector Store Retriever1": {
      "ai_retriever": [
        [
          {
            "node": "Question and Answer Chain1",
            "type": "ai_retriever",
            "index": 0
          }
        ]
      ]
    },
    "Supabase Vector Store5": {
      "ai_vectorStore": [
        [
          {
            "node": "Vector Store Retriever1",
            "type": "ai_vectorStore",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings Ollama4": {
      "ai_embedding": [
        [
          {
            "node": "Supabase Vector Store5",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model6": {
      "ai_languageModel": [
        [
          {
            "node": "Question and Answer Chain1",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "search_docs1": {
      "main": [
        [
          {
            "node": "Question and Answer Chain1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Edit Fields",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields": {
      "main": [
        [
          {
            "node": "Question and Answer Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model4": {
      "ai_languageModel": [
        [
          {
            "node": "Information Extractor",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Information Extractor": {
      "main": [
        [
          {
            "node": "Edit Fields1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Items1": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Theme Question",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields1": {
      "main": [
        [
          {
            "node": "Loop Over Items1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code": {
      "main": [
        [
          {
            "node": "Loop Over Items1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "Aggregate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model7": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate": {
      "main": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain1": {
      "main": [
        [
          {
            "node": "Loop Over Items2",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Loop Over Items2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser2": {
      "ai_outputParser": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Items2": {
      "main": [
        [
          {
            "node": "Replace Me",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Postgres",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Supabase": {
      "main": [
        []
      ]
    },
    "Postgres PGVector Store": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings Ollama1": {
      "ai_embedding": [
        [
          {
            "node": "Postgres PGVector Store",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Default Data Loader1": {
      "ai_document": [
        [
          {
            "node": "Postgres PGVector Store",
            "type": "ai_document",
            "index": 0
          }
        ]
      ]
    },
    "Character Text Splitter1": {
      "ai_textSplitter": [
        [
          {
            "node": "Default Data Loader1",
            "type": "ai_textSplitter",
            "index": 0
          }
        ]
      ]
    },
    "Postgres PGVector Store1": {
      "ai_vectorStore": [
        [
          {
            "node": "Vector Store Retriever",
            "type": "ai_vectorStore",
            "index": 0
          }
        ]
      ]
    },
    "Postgres": {
      "main": [
        [
          {
            "node": "Loop Over Items2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "bac7c1e9-e73a-456c-8c04-58da53e4a988",
  "meta": {
    "instanceId": "259d9a4393d46da31d322eb327090da890e9bf84cf7df0b6a92716d3eb5dd75b"
  },
  "id": "Lc3Nm9CJCb8g5rU8",
  "tags": []
}